// src/lib/uploadManager.js
import { supabase } from "@/lib/customSupabaseClient";
import { addTaskFile } from "@/api/taskFiles";

/**
 * Upload un fichier vers Supabase Storage avec le bon format de chemin
 * @param {File} file - Le fichier √† uploader
 * @param {string} taskId - ID de la t√¢che
 * @param {string} userId - ID de l'utilisateur (optionnel)
 * @returns {Promise<Object>} R√©sultat de l'upload avec URL publique
 */
export async function uploadTaskFile(file, taskId, userId = null) {
  try {
    // 1. Validation de la taille (50 Mo maximum)
    const MAX_FILE_SIZE = 50 * 1024 * 1024; // 50 Mo
    if (file.size > MAX_FILE_SIZE) {
      const sizeMB = (file.size / 1024 / 1024).toFixed(2);
      console.error(`‚ùå Fichier "${file.name}" trop volumineux: ${sizeMB} Mo (max: 50 Mo)`);
      return { 
        success: false, 
        error: `Fichier trop volumineux (${sizeMB} Mo). Limite: 50 Mo.` 
      };
    }

    // 2. Cr√©er le chemin selon le format attendu : tasks/{taskId}/{fileName}
    const timestamp = Date.now();
    const sanitizedFileName = file.name.replaceAll(/[^a-zA-Z0-9.-]/g, '_');
    const fileName = `${timestamp}_${sanitizedFileName}`;
    const filePath = `tasks/${taskId}/${fileName}`;

    console.log(`üì§ Upload du fichier "${file.name}" (${(file.size / 1024).toFixed(2)} Ko) pour la t√¢che ${taskId}...`);

    // 3. Uploader le fichier vers Supabase Storage (upload direct pour plus de vitesse)
    const { error: uploadError } = await supabase.storage
      .from("attachments")
      .upload(filePath, file, {
        cacheControl: "3600",
        upsert: true,
      });

    if (uploadError) {
      console.error("‚ùå Upload √©chou√©:", uploadError.message);
      
      // V√©rifier si c'est une erreur de bucket manquant
      if (uploadError.message?.includes("Bucket not found") || uploadError.message?.includes("bucket")) {
        console.error("‚ùå Le bucket 'attachments' n'existe pas dans Supabase Storage");
        // Essayer de cr√©er le bucket via RPC pr√©vu, puis r√©essayer l'upload une fois
        try {
          const { ensureAttachmentsBucket } = await import('@/lib/uploadManager');
          const created = await ensureAttachmentsBucket(false);
          if (created) {
            // retry upload once
            const { error: retryError } = await supabase.storage
              .from("attachments")
              .upload(filePath, file, {
                cacheControl: "3600",
                upsert: true,
              });
            if (retryError) {
              console.error("‚ùå R√©essai d'upload √©chou√©:", retryError.message);
              return { success: false, error: retryError.message };
            }
          } else {
            console.info("üëâ Cr√©ation automatique du bucket impossible. Cr√©ez-le manuellement : https://app.supabase.com/project/fhuzkubnxuetakpxkwlr/storage/buckets");
            return { 
              success: false, 
              error: "Bucket 'attachments' non configur√©. Cr√©ez-le dans Supabase Dashboard > Storage." 
            };
          }
        } catch (e) {
          console.warn("‚ö†Ô∏è Erreur lors de la tentative de cr√©ation du bucket :", e?.message || e);
          return { success: false, error: uploadError.message };
        }
      }
      
      return { success: false, error: uploadError.message };
    }

    console.log(`‚úÖ Upload vers Supabase Storage r√©ussi`);

    // 4. G√©n√©rer l'URL publique
    const { data: publicData } = supabase.storage
      .from("attachments")
      .getPublicUrl(filePath);

    const publicUrl = publicData?.publicUrl;

    if (!publicUrl) {
      console.error("‚ùå URL publique non g√©n√©r√©e pour le fichier:", file.name);
      return { success: false, error: "URL publique non g√©n√©r√©e" };
    }

    console.log(`‚úÖ URL publique g√©n√©r√©e: ${publicUrl}`);

    // 6. Backup local d√©sactiv√© ‚Äî √©viter conversion base64 c√¥t√© client qui peut
    // provoquer "Maximum call stack size exceeded" et ralentir l'upload.
    // Si un backup est n√©cessaire, l'UI devra l'impl√©menter avec readAsDataURL
    // et limites strictes. Ici nous n'envoyons jamais file_data.
    const base64Data = null;

    // 7. Enregistrer les m√©tadonn√©es dans la table tasks_files (avec backup base64 si disponible)
    console.log(`üíæ Enregistrement des m√©tadonn√©es dans tasks_files (task_id: ${taskId})...`);
    
    const fileRecord = await addTaskFile(
      taskId,
      file.name,
      publicUrl,
      file.size,
      file.type,
      userId,
      base64Data
    );

    if (!fileRecord.success) {
      console.error("‚ùå √âchec de l'enregistrement dans tasks_files:", fileRecord.error);
      return { 
        success: false, 
        error: `Fichier upload√© mais m√©tadonn√©es non sauvegard√©es: ${fileRecord.error?.message || fileRecord.error}` 
      };
    }

    console.log(`‚úÖ M√©tadonn√©es enregistr√©es (id: ${fileRecord.data?.id}) pour le fichier "${file.name}" li√© √† la t√¢che ${taskId}`);


    const result = {
      success: true,
      data: {
        id: fileRecord.data?.id || null,
        task_id: taskId,
        file_name: file.name,
        file_url: publicUrl,
        file_size: file.size,
        file_type: file.type,
        created_at: new Date().toISOString(),
        created_by: userId,
        is_accessible: true,
        valid_url: publicUrl
      }
    };
    
    console.log(`‚úÖ Fichier "${file.name}" enregistr√© et li√© √† la t√¢che ${taskId} ‚Äî ID: ${fileRecord.data?.id}`);
    return result;

  } catch (error) {
    console.error("‚ùå Erreur critique upload:", error);
    return { success: false, error: error.message };
  }
}

/**
 * Upload multiple files pour une t√¢che
 * @param {File[]} files - Array de fichiers √† uploader
 * @param {string} taskId - ID de la t√¢che
 * @param {string} userId - ID de l'utilisateur
 * @returns {Promise<Object>} R√©sultat des uploads
 */
export async function uploadMultipleTaskFiles(files, taskId, userId = null) {
  const results = {
    successes: [],
    errors: [],
    total: files.length
  };


  // Parallel uploads to improve speed. Each upload ensures storage upload completes
  // before calling addTaskFile (so we never insert DB rows for failed uploads).
  const promises = files.map(f => uploadTaskFile(f, taskId, userId));
  const settled = await Promise.allSettled(promises);

  for (const s of settled) {
    if (s.status === 'fulfilled') {
      const result = s.value;
      if (result && result.success) {
        results.successes.push(result.data);
      } else {
        results.errors.push({ fileName: result?.data?.file_name || 'unknown', error: result?.error || 'unknown' });
      }
    } else {
      results.errors.push({ fileName: 'unknown', error: s.reason?.message || String(s.reason) });
    }
  }


  const finalResult = {
    success: results.errors.length === 0,
    data: results.successes,
    errors: results.errors,
    summary: `${results.successes.length}/${results.total} fichiers upload√©s`
  };
  
  if (finalResult.success && results.successes.length > 0) {
    console.log(`‚úÖ ${results.successes.length} fichier(s) upload√©(s)`);
  }
  
  return finalResult;
}

/**
 * Assure que le bucket 'attachments' existe et est configur√© correctement.
 * 
 * üîí S√âCURIT√â :
 * Cette fonction utilise une approche RPC (Remote Procedure Call) pour cr√©er le bucket.
 * Si la fonction RPC n'existe pas, elle est cr√©√©e automatiquement via l'API SQL.
 * 
 * üöÄ AUTOCONFIGURATION COMPL√àTE :
 * 1. V√©rifie si le bucket existe
 * 2. D√©tecte si la fonction RPC 'create_attachments_bucket' existe
 * 3. Si absente, cr√©e automatiquement la fonction SQL avec SECURITY DEFINER
 * 4. Ex√©cute la fonction pour cr√©er le bucket
 * 5. Applique automatiquement les policies RLS pour l'acc√®s public
 * 
 * ‚úÖ AUCUNE INTERVENTION MANUELLE REQUISE
 * Le syst√®me s'autoconfigure au premier upload sans besoin d'acc√©der au Dashboard Supabase.
 * 
 * @param {boolean} silent - Si true, n'affiche pas les messages de log
 * @returns {Promise<boolean>} true si le bucket est pr√™t (existant ou cr√©√©)
 */
export async function ensureAttachmentsBucket(silent = false) {
  try {
    // Forcer la v√©rification √† chaque fois (pas de cache)
    // Le cache peut emp√™cher la d√©tection apr√®s cr√©ation manuelle du bucket
    const { data: buckets, error: listError } = await supabase.storage.listBuckets();
    
    if (listError) {
      if (!silent) {
        console.warn("‚ö†Ô∏è Impossible de lister les buckets:", listError.message);
      }
      return false;
    }

    const bucketExists = buckets?.some(bucket => bucket.name === 'attachments');

  if (!bucketExists) {
      // Tentative d'auto-cr√©ation via RPC s√©curis√© d√©j√† pr√©vu c√¥t√© SQL
      try {
        if (!silent) console.info("‚ö†Ô∏è Bucket 'attachments' non trouv√© ‚Äî appel RPC create_attachments_bucket()...");
        const { data: rpcData, error: rpcError } = await supabase.rpc('create_attachments_bucket');
        if (rpcError) {
          if (!silent) console.warn("‚ö†Ô∏è RPC create_attachments_bucket() √©chou√©e:", rpcError.message);
        } else {
          if (!silent) console.log("‚úÖ RPC ex√©cut√©e:", rpcData?.message || rpcData);
        }

        // Relister les buckets pour v√©rifier si la cr√©ation a r√©ussi
        const { data: newBuckets, error: newListError } = await supabase.storage.listBuckets();
        if (!newListError) {
          const nowExists = newBuckets?.some(b => b.name === 'attachments');
          if (nowExists) {
            if (!silent) console.log("‚úÖ Bucket 'attachments' d√©tect√© apr√®s RPC");
            return true;
          }
        }
      } catch (err) {
        if (!silent) console.error("‚ùå Erreur lors de la tentative RPC:", err?.message || err);
      }

      if (!silent) console.warn("‚ùå Le bucket 'attachments' est introuvable et la cr√©ation automatique a √©chou√©. Cr√©ez le bucket manuellement via Supabase SQL ou dashboard.");
      return false;
    }

    if (!silent) {
      console.log("‚úÖ Bucket 'attachments' pr√™t √† l'emploi");
    }
    return true;
  } catch (error) {
    if (!silent) {
      console.error("‚ùå Erreur lors de la v√©rification du bucket:", error.message);
    }
    return false;
  }
}

export async function deleteTaskFile(fileId, filePath) {
  try {
    const { error: storageError } = await supabase.storage
      .from("attachments")
      .remove([filePath]);

    if (storageError) {
      console.warn("Erreur lors de la suppression du storage :", storageError.message);
    }

    const { error: dbError } = await supabase
      .from("tasks_files")
      .delete()
      .eq("id", fileId);

    if (dbError) {
      return { success: false, error: dbError.message };
    }

    return { success: true };

  } catch (error) {
    return { success: false, error: error.message };
  }
}

export function getPublicUrl(filePath) {
  try {
    const { data } = supabase.storage
      .from("attachments")
      .getPublicUrl(filePath);

    return data?.publicUrl || null;
  } catch (error) {
    console.error("Erreur lors de la g√©n√©ration de l'URL publique :", error);
    return null;
  }
}

export async function initializeStorage() {
  console.log("üöÄ Initialisation du syst√®me de stockage Supabase...");
  const isReady = await ensureAttachmentsBucket(false);
  
  if (isReady) {
    console.log("‚úÖ Syst√®me de stockage initialis√© avec succ√®s");
  } else {
    console.warn("‚ö†Ô∏è Syst√®me de stockage partiellement initialis√©. Certaines fonctionnalit√©s peuvent √™tre limit√©es.");
  }
  
  return isReady;
}
